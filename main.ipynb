{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4f3a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from vietocr.tool.predictor import Predictor\n",
    "from vietocr.tool.config import Cfg\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cv2_imshow(img_array, save_path=\"temp_display_image.png\"):\n",
    "    \"\"\"Lưu ảnh tạm thay vì hiển thị trên màn hình\"\"\"\n",
    "    cv2.imwrite(save_path, img_array)\n",
    "    print(f\"Image saved to {save_path}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# === Khởi tạo EasyOCR ===\n",
    "reader = easyocr.Reader(['vi'])\n",
    "\n",
    "# === Khởi tạo model TableTransformer ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-structure-recognition-v1.1-all\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a1ab7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "overall_start_time = time.time()\n",
    "SHOW_PROCESSING_IMAGES: bool = False # Đặt thành False để không hiển thị ảnh\n",
    "\n",
    "DEFAULT_FONT = None\n",
    "font_paths_to_try = [\"DejaVuSans-Bold.ttf\", \"arial.ttf\", \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"]\n",
    "for font_path_item in font_paths_to_try:\n",
    "    try:\n",
    "        DEFAULT_FONT = ImageFont.truetype(font_path_item, 12)\n",
    "        break\n",
    "    except IOError:\n",
    "        continue\n",
    "if DEFAULT_FONT is None:\n",
    "    DEFAULT_FONT = ImageFont.load_default()\n",
    "\n",
    "def display_image(title: str, image_np: np.ndarray, max_width: int = 800) -> None:\n",
    "    if not SHOW_PROCESSING_IMAGES or image_np is None or image_np.size == 0:\n",
    "        return\n",
    "    h_orig, w_orig = image_np.shape[:2]\n",
    "    display_img_resized = image_np\n",
    "    if w_orig > max_width:\n",
    "        scale = max_width / w_orig\n",
    "        new_h = int(h_orig * scale)\n",
    "        display_img_resized = cv2.resize(image_np, (max_width, new_h), interpolation=cv2.INTER_AREA)\n",
    "    if lib_status[\"google_colab\"]:\n",
    "        cv2_imshow(display_img_resized)\n",
    "    else:\n",
    "        cv2.imshow(title, display_img_resized)\n",
    "        cv2.waitKey(1) # Cần thiết cho cv2.imshow hoạt động ngoài Colab\n",
    "    time.sleep(0.01)\n",
    "\n",
    "def display_image_with_bboxes(\n",
    "    title: str, image_np: np.ndarray,\n",
    "    cv_bbox_abs: tuple[int, int, int, int] | None = None,\n",
    "    tt_bbox_abs: tuple[int, int, int, int] | None = None,\n",
    "    max_width: int = 800\n",
    ") -> None:\n",
    "    if not SHOW_PROCESSING_IMAGES or image_np is None or image_np.size == 0: return\n",
    "    display_img_pil = Image.fromarray(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(display_img_pil)\n",
    "    if cv_bbox_abs:\n",
    "        x, y, w, h = cv_bbox_abs\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=\"blue\", width=3)\n",
    "        draw.text((x + 2, y + 2), \"CV Contour\", fill=\"blue\", font=DEFAULT_FONT)\n",
    "    if tt_bbox_abs:\n",
    "        x, y, w, h = tt_bbox_abs\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=3)\n",
    "        draw.text((x + 2, y - 15 if y > 15 else y + h + 2), \"TT Match\", fill=\"red\", font=DEFAULT_FONT)\n",
    "    display_img_np_annotated = cv2.cvtColor(np.array(display_img_pil), cv2.COLOR_RGB2BGR)\n",
    "    display_image(title, display_img_np_annotated, max_width)\n",
    "\n",
    "def display_processed_crop(title: str, processed_crop_np: np.ndarray, max_width: int = 800) -> None:\n",
    "    if not SHOW_PROCESSING_IMAGES or processed_crop_np is None or processed_crop_np.size == 0: return\n",
    "    display_image(title, processed_crop_np, max_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1dd8a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "yolo_model: 'YOLO | None' = None\n",
    "yolo_device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "yolo_model_classes: dict[int, str] = {}\n",
    "\n",
    "def load_yolo_model_safe(model_path_or_id: str, device_yolo: str) -> tuple['YOLO | None', dict[int, str]]:\n",
    "    global yolo_model_classes\n",
    "    if not model_path_or_id or not lib_status[\"ultralytics\"]:\n",
    "        return None, {}\n",
    "\n",
    "    loaded_model_local: 'YOLO | None' = None\n",
    "    model_classes_local: dict[int, str] = {}\n",
    "    try:\n",
    "        loaded_model_local = YOLO(model_path_or_id)\n",
    "        loaded_model_local.to(device_yolo)\n",
    "        dummy_img = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "        _ = loaded_model_local(dummy_img, device=(0 if device_yolo == 'cuda' else 'cpu'), verbose=False)\n",
    "\n",
    "        if hasattr(loaded_model_local, 'names') and loaded_model_local.names:\n",
    "            raw_names = loaded_model_local.names\n",
    "            if isinstance(raw_names, dict):\n",
    "                model_classes_local = {int(k): v for k, v in raw_names.items() if str(k).isdigit()}\n",
    "            elif isinstance(raw_names, (list, tuple)):\n",
    "                model_classes_local = {i: name for i, name in enumerate(raw_names)}\n",
    "        yolo_model_classes = model_classes_local\n",
    "    except Exception as e_yolo_load:\n",
    "        loaded_model_local = None\n",
    "        yolo_model_classes = {}\n",
    "    return loaded_model_local, model_classes_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3de0d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tt_image_processor_detection_global: 'AutoImageProcessor | None' = None\n",
    "tt_model_detection_global: 'TableTransformerForObjectDetection | None' = None\n",
    "\n",
    "if lib_status[\"transformers\"]:\n",
    "    try:\n",
    "        TT_DETECTION_MODEL_NAME = \"microsoft/table-transformer-detection\"\n",
    "        tt_image_processor_detection_global = AutoImageProcessor.from_pretrained(TT_DETECTION_MODEL_NAME)\n",
    "        tt_model_detection_global = TableTransformerForObjectDetection.from_pretrained(TT_DETECTION_MODEL_NAME)\n",
    "        tt_device_local = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if tt_model_detection_global is not None : tt_model_detection_global.to(tt_device_local)\n",
    "    except Exception as e_tt_load:\n",
    "        tt_model_detection_global = None\n",
    "        tt_image_processor_detection_global = None\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f5f38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def order_points(pts: np.ndarray) -> np.ndarray:\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\"); s = pts.sum(axis=1)\n",
    "    rect[0] = pts[np.argmin(s)]; rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[1] = pts[np.argmin(diff)]; rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "def get_largest_poly_from_mask(binary_mask_img: np.ndarray) -> np.ndarray | None:\n",
    "    contours, _ = cv2.findContours(binary_mask_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours: return None\n",
    "    largest_contour = max(contours, key=cv2.contourArea); peri = cv2.arcLength(largest_contour, True)\n",
    "    approx_poly = cv2.approxPolyDP(largest_contour, 0.02 * peri, True)\n",
    "    if len(approx_poly) == 4: return np.squeeze(approx_poly)\n",
    "    else:\n",
    "        rect = cv2.minAreaRect(largest_contour)\n",
    "        box_points = cv2.boxPoints(rect)\n",
    "        return np.int0(box_points)\n",
    "\n",
    "def get_max_dimensions(point_array: np.ndarray) -> tuple[int, int]:\n",
    "    if point_array is None or len(point_array) != 4: return 0, 0\n",
    "    rect_ordered = order_points(point_array.astype(np.float32)); (tl, tr, br, bl) = rect_ordered\n",
    "    width_a = np.sqrt(((br[0] - bl[0])**2) + ((br[1] - bl[1])**2))\n",
    "    width_b = np.sqrt(((tr[0] - tl[0])**2) + ((tr[1] - tl[1])**2))\n",
    "    max_width = max(int(width_a), int(width_b))\n",
    "    height_a = np.sqrt(((tr[0] - br[0])**2) + ((tr[1] - br[1])**2))\n",
    "    height_b = np.sqrt(((tl[0] - bl[0])**2) + ((tl[1] - bl[1])**2))\n",
    "    max_height = max(int(height_a), int(height_b))\n",
    "    return max_width, max_height\n",
    "\n",
    "def get_perspective_transform_matrix(point_array: np.ndarray, target_width: int, target_height: int) -> np.ndarray | None:\n",
    "    if point_array is None or len(point_array) != 4 or target_width <= 0 or target_height <= 0: return None\n",
    "    src_pts_ordered = order_points(point_array.astype(np.float32))\n",
    "    dst_pts_ordered = np.float32([[0,0],[target_width-1,0],[target_width-1,target_height-1],[0,target_height-1]])\n",
    "    try: return cv2.getPerspectiveTransform(src_pts_ordered, dst_pts_ordered)\n",
    "    except cv2.error: return None\n",
    "\n",
    "def warp_image_from_mask(image_bgr: np.ndarray, binary_mask_img: np.ndarray) -> np.ndarray:\n",
    "    if image_bgr is None or image_bgr.size==0 or binary_mask_img is None or binary_mask_img.size==0: return image_bgr\n",
    "    poly_coords_from_mask = get_largest_poly_from_mask(binary_mask_img)\n",
    "    if poly_coords_from_mask is None or len(poly_coords_from_mask) != 4: return image_bgr\n",
    "    max_w_target, max_h_target = get_max_dimensions(poly_coords_from_mask)\n",
    "    if max_w_target <= 10 or max_h_target <= 10: return image_bgr\n",
    "    transform_matrix_calc = get_perspective_transform_matrix(poly_coords_from_mask, max_w_target, max_h_target)\n",
    "    if transform_matrix_calc is None: return image_bgr\n",
    "    try:\n",
    "        warped_image_result = cv2.warpPerspective(image_bgr, transform_matrix_calc, (max_w_target, max_h_target), flags=cv2.INTER_LANCZOS4)\n",
    "        return warped_image_result if warped_image_result is not None and warped_image_result.size > 0 else image_bgr\n",
    "    except Exception: return image_bgr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5c7d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_image_safe(image: np.ndarray, angle: float,\n",
    "    border_val: tuple[int, int, int] | int | None = None,\n",
    "    border_mode: int = cv2.BORDER_REPLICATE\n",
    ") -> np.ndarray:\n",
    "    if image is None or image.size == 0 or abs(angle) < 0.01:\n",
    "        return image\n",
    "    try:\n",
    "        h, w = image.shape[:2]; center = (w // 2, h // 2)\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        if border_val is None: border_val = (255,255,255) if image.ndim==3 else 255\n",
    "        rotated_image = cv2.warpAffine(image,rotation_matrix,(w,h),flags=cv2.INTER_LANCZOS4,borderMode=border_mode,borderValue=border_val)\n",
    "        return rotated_image if rotated_image is not None and rotated_image.size > 0 else image\n",
    "    except Exception as e_rotate:\n",
    "        return image\n",
    "\n",
    "def deskew_page_basic(image_bgr: np.ndarray, angle_thresh_deg: float = 1.0,\n",
    "    bg_color: tuple[int, int, int] = (255, 255, 255)\n",
    ") -> np.ndarray:\n",
    "    if image_bgr is None or image_bgr.size == 0: return image_bgr\n",
    "\n",
    "    original_image_copy = image_bgr.copy(); h_orig, w_orig = image_bgr.shape[:2]\n",
    "    try:\n",
    "        gray_img = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        try: thresh_inv_img = cv2.adaptiveThreshold(gray_img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,21,10)\n",
    "        except cv2.error: _, thresh_inv_img = cv2.threshold(gray_img,0,255,cv2.THRESH_BINARY_INV|cv2.THRESH_OTSU)\n",
    "\n",
    "        kernel_width = max(15,int(w_orig*0.04)); kernel_height = max(3,int(h_orig*0.005))\n",
    "        morph_kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(kernel_width,kernel_height))\n",
    "        dilated_img = cv2.dilate(thresh_inv_img,morph_kernel,iterations=2)\n",
    "\n",
    "        edges_img = cv2.Canny(dilated_img,50,150,apertureSize=3)\n",
    "        if cv2.countNonZero(edges_img)==0: return original_image_copy\n",
    "\n",
    "        hough_line_thresh=max(50,int(w_orig*0.1)); min_line_length_hough=max(40,int(w_orig*0.08)); max_line_gap_hough=max(10,int(w_orig*0.02))\n",
    "        lines_detected = cv2.HoughLinesP(edges_img,1,np.pi/180,hough_line_thresh,minLineLength=min_line_length_hough,maxLineGap=max_line_gap_hough)\n",
    "\n",
    "        if lines_detected is None or len(lines_detected)==0: return original_image_copy\n",
    "\n",
    "        detected_angles_deg = [math.degrees(math.atan2(line[0][3]-line[0][1],line[0][2]-line[0][0]))\n",
    "                               for line in lines_detected\n",
    "                               if line[0][2]!=line[0][0] and abs(math.degrees(math.atan2(line[0][3]-line[0][1],line[0][2]-line[0][0])))<30.0]\n",
    "\n",
    "        if not detected_angles_deg: return original_image_copy\n",
    "\n",
    "        median_detected_angle = float(np.median(detected_angles_deg))\n",
    "        if not np.isfinite(median_detected_angle): return original_image_copy\n",
    "\n",
    "        correction_angle_val = -median_detected_angle\n",
    "\n",
    "        if abs(correction_angle_val) >= angle_thresh_deg:\n",
    "            deskewed_img_result = rotate_image_safe(original_image_copy, correction_angle_val, border_val=bg_color)\n",
    "            return deskewed_img_result\n",
    "        return original_image_copy\n",
    "    except Exception as e_deskew_page:\n",
    "        return original_image_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7918b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def deskew_table_precisely_v2(image_crop_bgr: np.ndarray, **dk_params) -> tuple[np.ndarray, float]:\n",
    "    if image_crop_bgr is None or image_crop_bgr.size==0: return image_crop_bgr,0.0\n",
    "    original_crop_copy = image_crop_bgr.copy(); h_crop,w_crop = original_crop_copy.shape[:2]\n",
    "    if h_crop<10 or w_crop<10: return original_crop_copy,0.0\n",
    "    applied_correction_angle=0.0; params=dk_params\n",
    "    try:\n",
    "        gray_crop = cv2.cvtColor(original_crop_copy,cv2.COLOR_BGR2GRAY)\n",
    "        try: bin_inv_crop = cv2.adaptiveThreshold(~gray_crop,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,15,-2)\n",
    "        except cv2.error: _,bin_inv_crop = cv2.threshold(~gray_crop,0,255,cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
    "\n",
    "        morph_kernel_w_factor = max(3,int(w_crop*params.get('morph_kernel_width_factor',0.15)))\n",
    "        h_line_kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(morph_kernel_w_factor,1))\n",
    "        h_lines_mask = cv2.morphologyEx(bin_inv_crop,cv2.MORPH_OPEN,h_line_kernel,iterations=params.get('morph_iterations',1))\n",
    "\n",
    "        if cv2.countNonZero(h_lines_mask)<(w_crop*0.05): return original_crop_copy,0.0\n",
    "\n",
    "        edges_from_h_mask = cv2.Canny(h_lines_mask,params.get('canny_low_thresh',50),params.get('canny_high_thresh',150),apertureSize=3)\n",
    "        if cv2.countNonZero(edges_from_h_mask)==0: return original_crop_copy,0.0\n",
    "\n",
    "        table_lines = cv2.HoughLinesP(edges_from_h_mask,1,np.pi/180,\n",
    "            threshold=params.get('hough_threshold',25),\n",
    "            minLineLength=max(15,int(w_crop*params.get('hough_min_line_length_factor',0.2))),\n",
    "            maxLineGap=max(5,int(w_crop*params.get('hough_max_line_gap_factor',0.05))) )\n",
    "\n",
    "        if table_lines is None or len(table_lines)==0: return original_crop_copy,0.0\n",
    "\n",
    "        angle_filter_degrees_param = params.get('angle_filter_degrees',25.0)\n",
    "        h_detected_angles = [math.degrees(math.atan2(l_seg[0][3]-l_seg[0][1],l_seg[0][2]-l_seg[0][0]))\n",
    "                             for l_seg in table_lines\n",
    "                             if l_seg[0][2]!=l_seg[0][0] and abs(math.degrees(math.atan2(l_seg[0][3]-l_seg[0][1],l_seg[0][2]-l_seg[0][0])))<=angle_filter_degrees_param]\n",
    "\n",
    "        if not h_detected_angles: return original_crop_copy,0.0\n",
    "\n",
    "        median_horizontal_angle = float(np.median(h_detected_angles))\n",
    "        if not np.isfinite(median_horizontal_angle): return original_crop_copy,0.0\n",
    "\n",
    "        h_correction_angle = -median_horizontal_angle\n",
    "\n",
    "        if abs(h_correction_angle)>params.get('rotation_threshold_degrees',0.2):\n",
    "            rotated_table_crop = rotate_image_safe(original_crop_copy,h_correction_angle)\n",
    "            applied_correction_angle = h_correction_angle\n",
    "            return rotated_table_crop, applied_correction_angle\n",
    "\n",
    "        return original_crop_copy,0.0\n",
    "    except Exception as e_deskew_table:\n",
    "        return original_crop_copy,0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c26988",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_iou_xywh(box1_xywh:tuple[int,int,int,int],box2_xywh:tuple[int,int,int,int])->float:\n",
    "    x1_i,y1_i,w1_i,h1_i=box1_xywh; x2_i,y2_i,w2_i,h2_i=box2_xywh\n",
    "    b1_x_min,b1_y_min,b1_x_max,b1_y_max = x1_i,y1_i,x1_i+w1_i,y1_i+h1_i\n",
    "    b2_x_min,b2_y_min,b2_x_max,b2_y_max = x2_i,y2_i,x2_i+w2_i,y2_i+h2_i\n",
    "    inter_x_min=max(b1_x_min,b2_x_min); inter_y_min=max(b1_y_min,b2_y_min)\n",
    "    inter_x_max=min(b1_x_max,b2_x_max); inter_y_max=min(b1_y_max,b2_y_max)\n",
    "    if inter_x_max<inter_x_min or inter_y_max<inter_y_min: return 0.0\n",
    "    intersection_area=(inter_x_max-inter_x_min)*(inter_y_max-inter_y_min)\n",
    "    b1_area=w1_i*h1_i; b2_area=w2_i*h2_i\n",
    "    union_area=float(b1_area+b2_area-intersection_area)\n",
    "    if union_area <= 0:\n",
    "        return 0.0 if intersection_area == 0 else 1.0\n",
    "    return intersection_area/union_area\n",
    "\n",
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.max_size = max_size\n",
    "    def __call__(self, image_pil):\n",
    "        width, height = image_pil.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        return image_pil.resize((int(round(scale * width)), int(round(scale * height))))\n",
    "\n",
    "structure_transform = transforms.Compose([\n",
    "    MaxResize(1000),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32, device=out_bbox.device)\n",
    "    return b\n",
    "\n",
    "def outputs_to_objects(outputs_model, img_size, id2label_map):\n",
    "    logits = outputs_model.logits.cpu() if outputs_model.logits.is_cuda else outputs_model.logits\n",
    "    pred_boxes = outputs_model.pred_boxes.cpu() if outputs_model.pred_boxes.is_cuda else outputs_model.pred_boxes\n",
    "    \n",
    "    m = logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().numpy())[0]\n",
    "    \n",
    "    pred_bboxes_scaled = [elem.tolist() for elem in rescale_bboxes(pred_boxes[0], img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes_scaled):\n",
    "        class_label = id2label_map[int(label)]\n",
    "        if class_label != \"no object\":\n",
    "            objects.append({'label': class_label, 'score': float(score), 'bbox': bbox})\n",
    "    return objects\n",
    "\n",
    "def get_cell_coordinates_by_row(table_data):\n",
    "    rows = [entry for entry in table_data if entry['label'] == 'table row']\n",
    "    columns = [entry for entry in table_data if entry['label'] == 'table column']\n",
    "    \n",
    "    rows.sort(key=lambda x: x['bbox'][1])\n",
    "    columns.sort(key=lambda x: x['bbox'][0])\n",
    "\n",
    "    def find_cell_coordinates(row_bbox, col_bbox):\n",
    "        return [col_bbox[0], row_bbox[1], col_bbox[2], row_bbox[3]]\n",
    "\n",
    "    cell_coords_by_row = []\n",
    "    for r_entry in rows:\n",
    "        row_cells_data = []\n",
    "        for c_entry in columns:\n",
    "            cell_bbox = find_cell_coordinates(r_entry['bbox'], c_entry['bbox'])\n",
    "            row_cells_data.append({'column_bbox': c_entry['bbox'], 'cell_bbox': cell_bbox})\n",
    "        \n",
    "        row_cells_data.sort(key=lambda x: x['column_bbox'][0])\n",
    "        cell_coords_by_row.append({'row_bbox': r_entry['bbox'], 'cells_data': row_cells_data, 'cell_count': len(row_cells_data)})\n",
    "    \n",
    "    cell_coords_by_row.sort(key=lambda x: x['row_bbox'][1])\n",
    "    return cell_coords_by_row\n",
    "\n",
    "def apply_ocr_to_cells(cell_coords_list, pil_image, reader_instance):\n",
    "    if reader_instance is None: return {} # Tránh lỗi nếu reader không được khởi tạo\n",
    "    ocr_data = dict()\n",
    "    max_cols = 0\n",
    "    \n",
    "    for r_idx, row_data_item in enumerate(tqdm(cell_coords_list, desc=\"OCR Rows\", leave=False) if 'tqdm' in globals() and SHOW_PROCESSING_IMAGES else cell_coords_list):\n",
    "        row_ocr_texts = []\n",
    "        for cell_info in row_data_item[\"cells_data\"]:\n",
    "            cropped_cell_pil = pil_image.crop(cell_info[\"cell_bbox\"])\n",
    "            cell_image_np = np.array(cropped_cell_pil)\n",
    "            \n",
    "            ocr_result = reader_instance.readtext(cell_image_np)\n",
    "            if ocr_result:\n",
    "                text = \" \".join([item[1] for item in ocr_result])\n",
    "                row_ocr_texts.append(text)\n",
    "            else:\n",
    "                row_ocr_texts.append(\"\")\n",
    "        \n",
    "        if len(row_ocr_texts) > max_cols:\n",
    "            max_cols = len(row_ocr_texts)\n",
    "        ocr_data[r_idx] = row_ocr_texts\n",
    "    \n",
    "    for r_key, r_val in ocr_data.items():\n",
    "        if len(r_val) != max_cols:\n",
    "            ocr_data[r_key] = r_val + [\"\"] * (max_cols - len(r_val))\n",
    "    return ocr_data\n",
    "\n",
    "\n",
    "def detect_tables_cv_tt_combined(\n",
    "    page_image_path:str|Path, output_dir_prefix:str,\n",
    "    cv_detect_params:dict, table_deskew_params:dict,\n",
    "    iou_match_threshold:float=0.4, tt_page_confidence_threshold:float=0.6\n",
    ") -> list[str]:\n",
    "    global tt_model_detection_global, tt_image_processor_detection_global, model_structure_recognition, device, ocr_reader\n",
    "    saved_crop_paths_list:list[str]=[]; page_file_path=Path(page_image_path); page_base_name=page_file_path.name\n",
    "    img_page_bgr_original_data=cv2.imread(str(page_file_path))\n",
    "    if img_page_bgr_original_data is None: return []\n",
    "\n",
    "    page_height,page_width = img_page_bgr_original_data.shape[:2]\n",
    "    if page_height<=10 or page_width<=10: return []\n",
    "\n",
    "    cv_candidate_bboxes_xywh_list:list[tuple[int,int,int,int]]=[]\n",
    "    try:\n",
    "        dt_params_cv=cv_detect_params; gray_page_img=cv2.cvtColor(img_page_bgr_original_data,cv2.COLOR_BGR2GRAY)\n",
    "        try:thresh_inv_page_img=cv2.adaptiveThreshold(~gray_page_img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,15,-2)\n",
    "        except:_,thresh_inv_page_img=cv2.threshold(~gray_page_img,0,255,cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
    "\n",
    "        kernel_h_width=max(20,int(page_width*dt_params_cv.get('min_line_ratio',0.02)))\n",
    "        kernel_v_height=max(20,int(page_height*dt_params_cv.get('min_line_ratio',0.02)))\n",
    "        kernel_h_lines=cv2.getStructuringElement(cv2.MORPH_RECT,(kernel_h_width,1))\n",
    "        kernel_v_lines=cv2.getStructuringElement(cv2.MORPH_RECT,(1,kernel_v_height))\n",
    "\n",
    "        opened_h_lines_mask=cv2.morphologyEx(thresh_inv_page_img,cv2.MORPH_OPEN,kernel_h_lines)\n",
    "        opened_v_lines_mask=cv2.morphologyEx(thresh_inv_page_img,cv2.MORPH_OPEN,kernel_v_lines)\n",
    "\n",
    "        combined_lines_mask=cv2.addWeighted(opened_h_lines_mask,0.5,opened_v_lines_mask,0.5,0.0)\n",
    "\n",
    "        dilate_iterations_cv=dt_params_cv.get('dilate_iter',2)\n",
    "        if dilate_iterations_cv > 0:\n",
    "            final_cv_mask = cv2.dilate(combined_lines_mask,cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)),iterations=dilate_iterations_cv)\n",
    "        else:\n",
    "            final_cv_mask = combined_lines_mask\n",
    "\n",
    "        if final_cv_mask is None or cv2.countNonZero(final_cv_mask)==0: raise ValueError(\"Mặt nạ OpenCV rỗng.\")\n",
    "\n",
    "        cv_contours_found,_=cv2.findContours(final_cv_mask,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not cv_contours_found: raise ValueError(\"Không tìm thấy đường viền nào bởi OpenCV.\")\n",
    "\n",
    "        min_area_pixels_cv=dt_params_cv.get('min_area_ratio',0.008)*page_width*page_height\n",
    "        min_dimension_pixels_cv=dt_params_cv.get('min_dim_px',25)\n",
    "\n",
    "        candidate_contours_data_list = []\n",
    "        for c_item in cv_contours_found:\n",
    "            x_c, y_c, w_c, h_c = cv2.boundingRect(c_item)\n",
    "            area_c = cv2.contourArea(c_item)\n",
    "            if area_c > min_area_pixels_cv and w_c > min_dimension_pixels_cv and h_c > min_dimension_pixels_cv:\n",
    "                candidate_contours_data_list.append({'bbox_xywh': (x_c, y_c, w_c, h_c), 'area': area_c})\n",
    "\n",
    "        if not candidate_contours_data_list: raise ValueError(\"Không có đường viền OpenCV nào vượt qua bộ lọc kích thước/diện tích.\")\n",
    "\n",
    "        candidate_contours_data_list.sort(key=lambda item:item['area'],reverse=True)\n",
    "\n",
    "        selected_indices_cv=set(); final_cv_candidates_list=[]\n",
    "        overlap_threshold_cv=dt_params_cv.get('overlap_threshold_ratio',0.7)\n",
    "\n",
    "        for i_idx in range(len(candidate_contours_data_list)):\n",
    "            if i_idx in selected_indices_cv: continue\n",
    "\n",
    "            current_contour_data=candidate_contours_data_list[i_idx]\n",
    "            final_cv_candidates_list.append(current_contour_data['bbox_xywh'])\n",
    "\n",
    "            for j_idx in range(i_idx + 1, len(candidate_contours_data_list)):\n",
    "                if j_idx in selected_indices_cv: continue\n",
    "\n",
    "                other_contour_data=candidate_contours_data_list[j_idx]\n",
    "                xA_overlap=max(current_contour_data['bbox_xywh'][0],other_contour_data['bbox_xywh'][0])\n",
    "                yA_overlap=max(current_contour_data['bbox_xywh'][1],other_contour_data['bbox_xywh'][1])\n",
    "                xB_overlap=min(current_contour_data['bbox_xywh'][0]+current_contour_data['bbox_xywh'][2],other_contour_data['bbox_xywh'][0]+other_contour_data['bbox_xywh'][2])\n",
    "                yB_overlap=min(current_contour_data['bbox_xywh'][1]+current_contour_data['bbox_xywh'][3],other_contour_data['bbox_xywh'][1]+other_contour_data['bbox_xywh'][3])\n",
    "                intersection_area_val=max(0,xB_overlap-xA_overlap)*max(0,yB_overlap-yA_overlap)\n",
    "\n",
    "                if intersection_area_val > 0 and (intersection_area_val/min(current_contour_data['area'],other_contour_data['area'])) > overlap_threshold_cv:\n",
    "                    selected_indices_cv.add(j_idx)\n",
    "\n",
    "        cv_candidate_bboxes_xywh_list = final_cv_candidates_list\n",
    "    except Exception as e_cv_detect: cv_candidate_bboxes_xywh_list=[]\n",
    "\n",
    "    tt_detected_bboxes_on_page_xywh_list:list[tuple[int,int,int,int]]=[]\n",
    "    if tt_model_detection_global and tt_image_processor_detection_global and lib_status[\"transformers\"]:\n",
    "        try:\n",
    "            pil_page_img = Image.fromarray(cv2.cvtColor(img_page_bgr_original_data,cv2.COLOR_BGR2RGB))\n",
    "            tt_inputs = tt_image_processor_detection_global(images=pil_page_img,return_tensors=\"pt\")\n",
    "\n",
    "            tt_model_device = next(tt_model_detection_global.parameters()).device\n",
    "            tt_inputs = {k_item:v_item.to(tt_model_device) for k_item,v_item in tt_inputs.items()}\n",
    "\n",
    "            with torch.no_grad(): tt_outputs = tt_model_detection_global(**tt_inputs)\n",
    "\n",
    "            tt_target_sizes=torch.tensor([pil_page_img.size[::-1]],device=tt_model_device)\n",
    "            tt_results_list = tt_image_processor_detection_global.post_process_object_detection(\n",
    "                tt_outputs, threshold=tt_page_confidence_threshold, target_sizes=tt_target_sizes\n",
    "            )[0]\n",
    "\n",
    "            min_dim_pixels_tt = cv_detect_params.get('min_dim_px',25)\n",
    "            for score_val,label_val,box_xyxy_val in zip(tt_results_list[\"scores\"],tt_results_list[\"labels\"],tt_results_list[\"boxes\"]):\n",
    "                if \"table\" in tt_model_detection_global.config.id2label[label_val.item()].lower():\n",
    "                    xmin_val,ymin_val,xmax_val,ymax_val=[int(round(c_coord.item())) for c_coord in box_xyxy_val]\n",
    "                    w_val,h_val=xmax_val-xmin_val,ymax_val-ymin_val\n",
    "                    if w_val > min_dim_pixels_tt and h_val > min_dim_pixels_tt:\n",
    "                        tt_detected_bboxes_on_page_xywh_list.append((xmin_val,ymin_val,w_val,h_val))\n",
    "        except Exception as e_tt_page_detect: tt_detected_bboxes_on_page_xywh_list=[]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    final_confirmed_pairs_data_list = []\n",
    "\n",
    "    if cv_candidate_bboxes_xywh_list and tt_detected_bboxes_on_page_xywh_list:\n",
    "        sorted_cv_bboxes_list = sorted(cv_candidate_bboxes_xywh_list,key=lambda b_item:(b_item[1],b_item[0]))\n",
    "\n",
    "        for cv_bbox_item in sorted_cv_bboxes_list:\n",
    "            best_iou_for_this_cv_bbox = 0.0\n",
    "            best_tt_match_for_this_cv_bbox = None\n",
    "\n",
    "            for tt_bbox_item in tt_detected_bboxes_on_page_xywh_list:\n",
    "                iou_val = calculate_iou_xywh(cv_bbox_item, tt_bbox_item)\n",
    "                if iou_val > best_iou_for_this_cv_bbox:\n",
    "                    best_iou_for_this_cv_bbox = iou_val\n",
    "                    best_tt_match_for_this_cv_bbox = tt_bbox_item\n",
    "\n",
    "            if best_iou_for_this_cv_bbox > iou_match_threshold and best_tt_match_for_this_cv_bbox is not None:\n",
    "                final_confirmed_pairs_data_list.append({\n",
    "                    \"cv_bbox\": cv_bbox_item,\n",
    "                    \"tt_bbox\": best_tt_match_for_this_cv_bbox,\n",
    "                    \"iou\": best_iou_for_this_cv_bbox\n",
    "                })\n",
    "\n",
    "    if not final_confirmed_pairs_data_list:\n",
    "        return []\n",
    "\n",
    "    processed_table_count = 0\n",
    "    for pair_index, pair_item_data in enumerate(final_confirmed_pairs_data_list):\n",
    "        cv_bbox_to_process_item = pair_item_data[\"cv_bbox\"]\n",
    "        matched_tt_bbox_absolute = pair_item_data[\"tt_bbox\"]\n",
    "        iou_score_val = pair_item_data[\"iou\"]\n",
    "\n",
    "        if SHOW_PROCESSING_IMAGES:\n",
    "            display_image_with_bboxes(\n",
    "                title=f\"Trang {page_base_name} - Khớp {pair_index+1} (IoU {iou_score_val:.2f}) - Bối cảnh trang gốc\",\n",
    "                image_np=img_page_bgr_original_data,\n",
    "                cv_bbox_abs=cv_bbox_to_process_item,\n",
    "                tt_bbox_abs=matched_tt_bbox_absolute\n",
    "            )\n",
    "\n",
    "        x_cv_coord, y_cv_coord, w_cv_dim, h_cv_dim = cv_bbox_to_process_item\n",
    "        padding_val = cv_detect_params.get('padding',5)\n",
    "\n",
    "        y1_crop_coord = max(0, y_cv_coord - padding_val)\n",
    "        y2_crop_coord = min(page_height, y_cv_coord + h_cv_dim + padding_val)\n",
    "        x1_crop_coord = max(0, x_cv_coord - padding_val)\n",
    "        x2_crop_coord = min(page_width, x_cv_coord + w_cv_dim + padding_val)\n",
    "\n",
    "        if (x2_crop_coord - x1_crop_coord) <= 10 or (y2_crop_coord - y1_crop_coord) <= 10:\n",
    "            continue\n",
    "\n",
    "        cv_crop_bgr_padded_img = img_page_bgr_original_data[y1_crop_coord:y2_crop_coord, x1_crop_coord:x2_crop_coord].copy()\n",
    "        if cv_crop_bgr_padded_img is None or cv_crop_bgr_padded_img.size == 0:\n",
    "            continue\n",
    "\n",
    "        deskewed_table_crop_img = cv_crop_bgr_padded_img.copy()\n",
    "        try:\n",
    "            deskewed_output_img, applied_angle_val = deskew_table_precisely_v2(cv_crop_bgr_padded_img, **table_deskew_params)\n",
    "            if deskewed_output_img is not None and deskewed_output_img.size > 0:\n",
    "                deskewed_table_crop_img = deskewed_output_img\n",
    "                if abs(applied_angle_val) > 0.01:\n",
    "                     pass\n",
    "        except Exception as e_deskew_crop:\n",
    "            pass\n",
    "\n",
    "        final_processed_crop_for_saving_img = deskewed_table_crop_img.copy()\n",
    "        try:\n",
    "            if deskewed_table_crop_img is not None and deskewed_table_crop_img.size > 0:\n",
    "                gray_for_warp_mask_img = cv2.cvtColor(deskewed_table_crop_img, cv2.COLOR_BGR2GRAY)\n",
    "                _, bin_for_warp_mask_img = cv2.threshold(gray_for_warp_mask_img, 0, 255,\n",
    "                                                    cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "                warped_output_img = warp_image_from_mask(deskewed_table_crop_img, bin_for_warp_mask_img)\n",
    "                if warped_output_img is not None and warped_output_img.size > 0 and \\\n",
    "                   warped_output_img.shape[0] > 10 and warped_output_img.shape[1] > 10:\n",
    "                    final_processed_crop_for_saving_img = warped_output_img\n",
    "        except Exception as e_warp_crop:\n",
    "            pass\n",
    "        \n",
    "        if final_processed_crop_for_saving_img is not None and final_processed_crop_for_saving_img.size > 0:\n",
    "            temp_crop_file_path = None\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
    "                    temp_crop_file_path = tmpfile.name\n",
    "                \n",
    "                if not cv2.imwrite(temp_crop_file_path, final_processed_crop_for_saving_img):\n",
    "                    if temp_crop_file_path and os.path.exists(temp_crop_file_path):\n",
    "                         os.remove(temp_crop_file_path)\n",
    "                    temp_crop_file_path = None\n",
    "                    continue \n",
    "\n",
    "                image_pil_from_temp = Image.open(temp_crop_file_path).convert(\"RGB\")\n",
    "                \n",
    "                pixel_values = structure_transform(image_pil_from_temp).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model_structure_recognition(pixel_values)\n",
    "\n",
    "                structure_id2label = model_structure_recognition.config.id2label.copy()\n",
    "                structure_id2label[len(structure_id2label)] = \"no object\"\n",
    "                \n",
    "                cells = outputs_to_objects(outputs, image_pil_from_temp.size, structure_id2label)\n",
    "                cell_coordinates_data = get_cell_coordinates_by_row(cells)\n",
    "                data_ocr = apply_ocr_to_cells(cell_coordinates_data, image_pil_from_temp, ocr_reader)\n",
    "                import csv\n",
    "                with open(\"table_output.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "                  writer = csv.writer(f)\n",
    "                  for row in data_ocr.values():\n",
    "                      writer.writerow(row)\n",
    "                      \n",
    "                for row_values in data_ocr.values():\n",
    "                    pass \n",
    "\n",
    "                safe_prefix_filename = \"\".join(c_char if c_char.isalnum() or c_char in ('_', '-') else '_' for c_char in Path(output_dir_prefix).name)\n",
    "                output_table_filename = f\"{safe_prefix_filename}_table_CVmatchTT_{processed_table_count:02d}_iou{iou_score_val:.2f}.png\"\n",
    "                output_table_path = Path(Path(output_dir_prefix).parent) / output_table_filename\n",
    "\n",
    "                try:\n",
    "                    if cv2.imwrite(str(output_table_path), final_processed_crop_for_saving_img):\n",
    "                        saved_crop_paths_list.append(str(output_table_path))\n",
    "                    else:\n",
    "                        pass\n",
    "                except Exception as e_save_table:\n",
    "                    pass\n",
    "            \n",
    "            except Exception as e_processing:\n",
    "                pass\n",
    "            finally:\n",
    "                if temp_crop_file_path and os.path.exists(temp_crop_file_path):\n",
    "                    try:\n",
    "                        os.remove(temp_crop_file_path)\n",
    "                    except OSError:\n",
    "                        pass\n",
    "            processed_table_count +=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return saved_crop_paths_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9f928",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Pipe_line\n",
    "\n",
    "def process_pdf_for_tables_pipeline(\n",
    "    pdf_file_path_main:str|Path, base_output_dir_prefix:str, pdf_processing_dpi:int, enable_pdf_page_deskew:bool,\n",
    "    pdf_page_deskew_angle_thresh:float, cv_detection_config_main:dict, table_deskew_config_main:dict,\n",
    "    iou_matching_thresh_config:float, tt_page_confidence_thresh_config_val:float, temp_pdf_pages_dir_path:str|Path\n",
    ") -> tuple[dict[int,list[str]],int]:\n",
    "\n",
    "    all_page_processing_results:dict[int,list[str]]={}; total_tables_extracted_count=0\n",
    "\n",
    "    pdf_path_obj=Path(pdf_file_path_main)\n",
    "    temp_pages_dir_obj=Path(temp_pdf_pages_dir_path)\n",
    "\n",
    "    shutil.rmtree(temp_pages_dir_obj, ignore_errors=True)\n",
    "    os.makedirs(temp_pages_dir_obj, exist_ok=True)\n",
    "\n",
    "    output_main_dir=Path(base_output_dir_prefix).parent\n",
    "    if not output_main_dir.exists():\n",
    "        os.makedirs(output_main_dir, exist_ok=True)\n",
    "\n",
    "    if not lib_status[\"pdf2image\"]:\n",
    "        return {}, 0\n",
    "\n",
    "    pdf_page_pil_images_list:list[Image.Image]=[]\n",
    "    try:\n",
    "        start_conversion_time=time.time(); cpu_cores_count=os.cpu_count()or 4; thread_count_val=max(1,cpu_cores_count//2)\n",
    "        pdf_page_pil_images_list = convert_from_path(\n",
    "            pdf_path_obj, dpi=pdf_processing_dpi, fmt='png',\n",
    "            thread_count=thread_count_val, use_pdftocairo=True\n",
    "        )\n",
    "        conversion_duration_sec=time.time()-start_conversion_time\n",
    "        if not pdf_page_pil_images_list:\n",
    "            raise ValueError(\"Chuyển đổi PDF cho ra 0 hình ảnh.\")\n",
    "    except Exception as e_pdf_convert:\n",
    "        shutil.rmtree(temp_pages_dir_obj, ignore_errors=True)\n",
    "        return {}, 0\n",
    "\n",
    "    for current_page_idx, current_pil_image in enumerate(pdf_page_pil_images_list):\n",
    "        current_page_num = current_page_idx + 1\n",
    "        temp_page_image_file_path:Path|None=None\n",
    "\n",
    "        try:\n",
    "            cv_bgr_page_image = cv2.cvtColor(np.array(current_pil_image), cv2.COLOR_RGB2BGR)\n",
    "            if cv_bgr_page_image is None or cv_bgr_page_image.size == 0:\n",
    "                all_page_processing_results[current_page_num] = []\n",
    "                continue\n",
    "\n",
    "            image_to_process_for_saving = cv_bgr_page_image.copy()\n",
    "\n",
    "            if enable_pdf_page_deskew:\n",
    "                deskewed_page_version = deskew_page_basic(cv_bgr_page_image, pdf_page_deskew_angle_thresh)\n",
    "                image_to_process_for_saving = deskewed_page_version\n",
    "\n",
    "            temp_page_image_filename = f\"page_{current_page_num:03d}.png\"\n",
    "            temp_page_image_file_path = temp_pages_dir_obj / temp_page_image_filename\n",
    "\n",
    "            if not cv2.imwrite(str(temp_page_image_file_path), image_to_process_for_saving):\n",
    "                all_page_processing_results[current_page_num] = []\n",
    "                continue\n",
    "\n",
    "            current_page_output_prefix = f\"{base_output_dir_prefix}_pg{current_page_num:03d}\"\n",
    "\n",
    "            saved_table_paths_for_this_page = detect_tables_cv_tt_combined(\n",
    "                page_image_path=temp_page_image_file_path, output_dir_prefix=current_page_output_prefix,\n",
    "                cv_detect_params=cv_detection_config_main, table_deskew_params=table_deskew_config_main,\n",
    "                iou_match_threshold=iou_matching_thresh_config, tt_page_confidence_threshold=tt_page_confidence_thresh_config_val\n",
    "            )\n",
    "\n",
    "            all_page_processing_results[current_page_num] = saved_table_paths_for_this_page\n",
    "            num_tables_on_this_page = len(saved_table_paths_for_this_page)\n",
    "            total_tables_extracted_count += num_tables_on_this_page\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            shutil.rmtree(temp_pages_dir_obj, ignore_errors=True)\n",
    "            raise\n",
    "        except Exception as e_process_page:\n",
    "            import traceback\n",
    "            all_page_processing_results[current_page_num] = []\n",
    "        finally:\n",
    "            if temp_page_image_file_path and temp_page_image_file_path.exists():\n",
    "                try: os.remove(temp_page_image_file_path)\n",
    "                except OSError as e_remove_temp: pass\n",
    "\n",
    "    shutil.rmtree(temp_pages_dir_obj, ignore_errors=True)\n",
    "    return all_page_processing_results, total_tables_extracted_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a85280",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"PDF_DPI\":200,\n",
    "    \"ENABLE_PAGE_DESKEW\":True,\n",
    "    \"PAGE_DESKEW_ANGLE_THRESHOLD\":0.5,\n",
    "    \"CV_DETECT_PARAMS\":{\n",
    "        'min_line_ratio':0.015,\n",
    "        'min_area_ratio':0.005,\n",
    "        'min_dim_px':20,\n",
    "        'dilate_iter':2,\n",
    "        'padding':10,\n",
    "        'overlap_threshold_ratio':0.65\n",
    "    },\n",
    "    \"TABLE_DESKEW_PARAMS\":{\n",
    "        'morph_kernel_width_factor':0.12,\n",
    "        'morph_iterations':1,\n",
    "        'hough_threshold':20,\n",
    "        'hough_min_line_length_factor':0.15,\n",
    "        'hough_max_line_gap_factor':0.04,\n",
    "        'canny_low_thresh':40,\n",
    "        'canny_high_thresh':120,\n",
    "        'angle_filter_degrees':20.0,\n",
    "        'rotation_threshold_degrees':0.20\n",
    "    },\n",
    "    \"IOU_MATCH_THRESHOLD\":0.35,\n",
    "    \"TT_PAGE_CONFIDENCE_THRESHOLD\":0.9,\n",
    "    \"OUTPUT_BASE_DIR\":Path(\"./table_extraction_output_cv_priority\"), # Sửa thành ./ cho local\n",
    "    \"TEMP_PDF_PAGES_DIR\":Path(\"./temp_pdf_pages_cv_priority\"), # Sửa thành ./ cho local\n",
    "    \"YOLO_ENABLE_VALIDATION\":False,\n",
    "    \"YOLO_MODEL_PATH\":\"keremberke/yolov8n-table-detection\",\n",
    "}\n",
    "\n",
    "if CONFIG[\"YOLO_ENABLE_VALIDATION\"] and lib_status[\"ultralytics\"]:\n",
    "    yolo_model, _ = load_yolo_model_safe(CONFIG[\"YOLO_MODEL_PATH\"],yolo_device)\n",
    "    if yolo_model:\n",
    "        pass\n",
    "elif CONFIG[\"YOLO_ENABLE_VALIDATION\"]:\n",
    "    pass\n",
    "\n",
    "shutil.rmtree(CONFIG[\"OUTPUT_BASE_DIR\"],ignore_errors=True)\n",
    "shutil.rmtree(CONFIG[\"TEMP_PDF_PAGES_DIR\"],ignore_errors=True)\n",
    "os.makedirs(CONFIG[\"OUTPUT_BASE_DIR\"],exist_ok=True)\n",
    "os.makedirs(CONFIG[\"TEMP_PDF_PAGES_DIR\"],exist_ok=True)\n",
    "\n",
    "\n",
    "uploaded_pdf_file_path_main:Path|None=None\n",
    "if lib_status[\"google_colab\"]:\n",
    "    try:\n",
    "        content_main_dir=Path(\"/content/\")\n",
    "        for item_file in content_main_dir.iterdir():\n",
    "            if item_file.is_file() and not item_file.name.startswith('.'):\n",
    "                try: item_file.unlink()\n",
    "                except OSError: pass\n",
    "\n",
    "        uploaded_colab_files = files.upload()\n",
    "        if uploaded_colab_files:\n",
    "            uploaded_pdf_filename=list(uploaded_colab_files.keys())[0]\n",
    "            source_pdf_path_in_colab_runtime = content_main_dir / uploaded_pdf_filename\n",
    "\n",
    "            if uploaded_pdf_filename.lower().endswith(\".pdf\"):\n",
    "                if not source_pdf_path_in_colab_runtime.exists():\n",
    "                    raise FileNotFoundError(\"Tệp PDF đã tải lên dường như đã biến mất ngay lập tức.\")\n",
    "\n",
    "                final_pdf_destination_path = CONFIG[\"OUTPUT_BASE_DIR\"] / uploaded_pdf_filename\n",
    "                try:\n",
    "                    shutil.move(str(source_pdf_path_in_colab_runtime), str(final_pdf_destination_path))\n",
    "                    uploaded_pdf_file_path_main = final_pdf_destination_path\n",
    "                except Exception as e_move_pdf:\n",
    "                    uploaded_pdf_file_path_main = source_pdf_path_in_colab_runtime\n",
    "            else:\n",
    "                if source_pdf_path_in_colab_runtime.exists(): source_pdf_path_in_colab_runtime.unlink()\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e_upload_pdf:\n",
    "        pass\n",
    "else:\n",
    "    # Để kiểm thử cục bộ, bạn có thể đặt `uploaded_pdf_file_path_main` thủ công ở đây:\n",
    "    local_test_pdf_path = Path(\"./example.pdf\") # << THAY ĐỔI ĐƯỜNG DẪN NÀY\n",
    "    if local_test_pdf_path.exists() and local_test_pdf_path.is_file() and local_test_pdf_path.name.lower().endswith(\".pdf\"):\n",
    "        # Copy file vào output dir để giữ file gốc\n",
    "        destination_in_output = CONFIG[\"OUTPUT_BASE_DIR\"] / local_test_pdf_path.name\n",
    "        try:\n",
    "            shutil.copy2(local_test_pdf_path, destination_in_output)\n",
    "            uploaded_pdf_file_path_main = destination_in_output\n",
    "        except Exception:\n",
    "             uploaded_pdf_file_path_main = local_test_pdf_path # Fallback to original if copy fails\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "if not uploaded_pdf_file_path_main or not uploaded_pdf_file_path_main.exists():\n",
    "    sys.exit(\"Thực thi bị dừng: Không có PDF hợp lệ được cung cấp hoặc tìm thấy.\")\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7415f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                 *** MAIN ***\n",
    "# ==============================================================================\n",
    "if uploaded_pdf_file_path_main and uploaded_pdf_file_path_main.exists():\n",
    "    pdf_basename_val=uploaded_pdf_file_path_main.name\n",
    "    pdf_name_no_ext_val=uploaded_pdf_file_path_main.stem\n",
    "\n",
    "    current_pdf_output_base_prefix = str(CONFIG[\"OUTPUT_BASE_DIR\"] / pdf_name_no_ext_val)\n",
    "\n",
    "    processing_start_main_time = time.time()\n",
    "    try:\n",
    "        page_results_dict, total_tables_extracted_final_count = process_pdf_for_tables_pipeline(\n",
    "            pdf_file_path_main=uploaded_pdf_file_path_main,\n",
    "            base_output_dir_prefix=current_pdf_output_base_prefix,\n",
    "            pdf_processing_dpi=CONFIG[\"PDF_DPI\"],\n",
    "            enable_pdf_page_deskew=CONFIG[\"ENABLE_PAGE_DESKEW\"],\n",
    "            pdf_page_deskew_angle_thresh=CONFIG[\"PAGE_DESKEW_ANGLE_THRESHOLD\"],\n",
    "            cv_detection_config_main=CONFIG[\"CV_DETECT_PARAMS\"],\n",
    "            table_deskew_config_main=CONFIG[\"TABLE_DESKEW_PARAMS\"],\n",
    "            iou_matching_thresh_config=CONFIG[\"IOU_MATCH_THRESHOLD\"],\n",
    "            tt_page_confidence_thresh_config_val=CONFIG[\"TT_PAGE_CONFIDENCE_THRESHOLD\"],\n",
    "            temp_pdf_pages_dir_path=CONFIG[\"TEMP_PDF_PAGES_DIR\"]\n",
    "        )\n",
    "\n",
    "        processing_duration_main_sec = time.time() - processing_start_main_time\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except Exception as e_main_process:\n",
    "        import traceback\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
